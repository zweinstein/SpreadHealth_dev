{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with the TFxIDF vectorization and logistic regression with gradient descent, the advantages of hashing vectorizer and logistric regression with stochastic gradient descent are:\n",
    "1. It allows scalable out-of-core online learning, to handle much larger text dataset. \n",
    "2. It also integrates better with the Flask App, since it doesn't require storing the vocabulory dictionary in memory for new text vectorization, and can update the classifier with new text data to improve future performance.\n",
    "3. Faster and easier to pickle. \n",
    "\n",
    "The disadvantages of hashing vectorizer are:\n",
    "1. Cannot introspect which features are most important to a model, since it does not store the vocabulory dictionary.\n",
    "2. There can be collisions: distinct tokens can be mapped to the same feature index. However this should not be an issue if n_features is large enough (e.g. 2<sup>21</sup> here).\n",
    "3. No IDF weighting.\n",
    "\n",
    "Therefore, I used TFxIDF vectorization and logistic regression with gradient descent for the development stage of the project (logit_GD), and switched to hashing vectorizer with the same model (logistic regression) and similar parameters for the production stage of the project (this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from __future__ import print_function # Use print as a function like in Python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "def tweetTokenizer(tweet):\n",
    "    tw = [w for w in tweet.split() if w not in stop]\n",
    "    return tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            t = line.split(',')\n",
    "            text, label = t[1], int(t[0])\n",
    "            yield text, label\n",
    "\n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define current working directory as work_dir\n",
    "os.chdir(work_dir)\n",
    "path = 'rtBinaryClean.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(path, \"r\") as f:\n",
    "    next(f) # skip header\n",
    "    for line in f:\n",
    "        text = line.split(',')[1]\n",
    "        label = int(line.split(',')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tweetTokenizer)\n",
    "\n",
    "clf = SGDClassifier(loss='log', penalty='l2', alpha=0.00004, random_state=1, n_iter=1)\n",
    "doc_stream = stream_docs('rtBinaryClean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%     100%\n",
      "[#########] | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(9)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(9):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.717\n",
      "Random Chance Accuracy in Test Set: 0.463\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))\n",
    "clf = clf.partial_fit(X_test, y_test)\n",
    "pct = float(sum(y_test))/len(y_test)\n",
    "print('Random Chance Accuracy in Test Set: %.3f' % pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle the stopword list and classifier for Flask App to use online:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "dest = os.path.join(cwd, 'models', 'pkl_obj')\n",
    "\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "with open(os.path.join(dest, 'stopwords.pkl'),'wb') as f:\n",
    "    pickle.dump(stop, f)\n",
    "with open(os.path.join(dest, 'classifier.pkl'),'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [insight_env]",
   "language": "python",
   "name": "Python [insight_env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
